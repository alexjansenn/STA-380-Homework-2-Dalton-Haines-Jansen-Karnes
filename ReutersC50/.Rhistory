my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))
DTM = DocumentTermMatrix(my_corpus)
getwd()
setwd(/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50)
setwd('/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50')
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
author_dirs = Sys.glob('C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))
DTM = DocumentTermMatrix(my_corpus)
DTM # some basic summary statistics
class(DTM)  # a special kind of sparse matrix format
inspect(DTM[1:2500,1:50])
DTM = removeSparseTerms(DTM, 0.975)
DTM
X = as.matrix(DTM)
n <- 50
nr <- nrow(X)
author_train = cbind(split(X, rep(1:ceiling(nr/n), each=n, length.out=nr)))
View(author_train)
author_train[1]
AP_train = X[1:45,]
write.csv(author_train, file = "author_train.csv")
at= split(X, rep(1:ceiling(nr/n), each=n, length.out=nr))
print (X[i:i+50,])
for(i in chunk(from = 1, to = nrow(X), by = 50)){
print (X[i:i+50,])
}
install.packages('chunk')
split(X, rep(1:50, each = round(NROW(df) / 100, -4)))
split(X, rep(1:50, each = round(nrow(X) / 100, -4)))
?split
split(X, rep(1:50, each = 1:ceiling(nr/n)))
abc= split(X, rep(1:50, each = 1:ceiling(nr/n)))
n <- 50
nr <- nrow(X)
abc= split(X, rep(1:50, each = 1:ceiling(nr/n)))
for(i in seq(1,nrow(X), by = 50)){
print '(X[i:i+50,])'
}
for(i in seq(1,nrow(X), by = 50)){
print '(X[i:i+50,])'
}
for(i in seq(1,nrow(X), by = 50)){
print '(X[i:i+50,])'
}
for(i in seq(1,nrow(X), by = 50)){
print 'hi'
}
for(i in seq(1,nrow(X), by = 50)){
print 'hi'
}
for(i in seq(1,nrow(X), by = 50)) print 'hi'
print('hi')
n <- 50
nr <- nrow(X)
for(i in seq(1,1000, by = 50)){
i:i+49
}
for(i in seq(1,1000, by = 50)){
x= cbind(i:i+49)
}
View(x)
x= cbind([i:i+49,])
num_words=dim(X)[2]
num_words
w=matrix(0,50,num_words)
w
smooth_count=1/nrow(X)
smooth_count
?colSums
num_words=dim(X)[2]
w=matrix(0,50,num_words)
smooth_count=1/nrow(X)
count=1
i=1
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =author_matrix
count=count+50
i=i+1
}
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =colsums(author_matrix)
count=count+50
i=i+1
}
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =colSums(author_matrix)
count=count+50
i=i+1
}
View(w)
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =author_matrix
count=count+50
i=i+1
}
num_words=dim(X)[2]
w=matrix(0,50,num_words)
smooth_count=1/nrow(X)
count=1
i=1
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =author_matrix
count=count+50
i=i+1
}
author_matrix =matrix(X[1:50],50,num_words)
hi =matrix(X[1:50],50,num_words)
View(hi)
num_words
while(count<=2500){
author_matrix =matrix(X[count:count+49],50,)
w[i,] =author_matrix
count=count+50
i=i+1
}
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =X[count:count+49]
count=count+50
i=i+1
}
View(w)
at[1]
at[1][5]
at[1]$5
at[1]$1
at[1]$`1`
zx = cbind(split(X, rep(1:ceiling(nrow(X)/50), each=50, length.out=nrow(X))))
View(zx)
View(w)
count=1
i=1
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =X[count:count+49]
count=count+50
i=i+1
}
View(w)
w=matrix(0,50,num_words)
w
View(w)
X[1:45,]
vg = X[1:45,]
View(vg)
count=1
i=1
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =X[count:count+49]
count=count+50
i=i+1
}
w[1]
?matrix
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[i,] =X[count:count+49,]
count=count+50
i=i+1
}
w[1]
w[1,1]
w[1,2]
w[1,:]
w[1,]
sum(w[1,])
X[1:45,]
k = 50; n=50; m = num_words
big_array =array(NA, c(n,m,k))
big_array[1]
big_array[1,]
big_array[1, ,]
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
big_array[i, ,] =X[count:count+49,]
count=count+50
i=i+1
}
View(X)
?array
big_array[1,]
big_array[1, ,]
View(AP_train)
k = 50; m=50; n = num_words
big_array =array(NA, c(n,m,k))
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
big_array[i, ,] =X[count:count+49,]
count=count+50
i=i+1
}
big_array[1, , ]
k = 50; n=50; m = num_words
big_array =array(NA, c(n,m,k))
k = num_words; n=50; m = 50
big_array =array(NA, c(n,m,k))
big_array[1, ,]
m = num_words; n=50; k = 50
big_array =array(NA, c(n,m,k))
dim(big_array)
n = num_words; m=50; k = 50
big_array =array(NA, c(n,m,k))
big_array[1, ,]
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
big_array[i, ,] =X[count:count+49,]
count=count+50
i=i+1
}
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
big_array[,i ,] =X[count:count+49,]
count=count+50
i=i+1
}
X[1:50]$Docs
X[1:50,]$Docs
X[1:50,]
author_matrix =matrix(X[1:50],50,num_words)
author_matrix
author_matrix =matrix(X[1:50,],50,num_words)
author_matrix
author_matrix =matrix(50,X[1:50,],50)
author_matrix
author_matrix =matrix(X[1:50,],50)
author_matrix
dim(X)
m = num_words; n=50; k = 50
big_array =array(NA, c(n,m,k))
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
big_array[, ,i] =X[count:count+49,]
count=count+50
i=i+1
}
big_array[,,1]
big_array[,1,]
?rbind
?cbind
mydata <- list()
while(count<=2500){
# author_matrix =matrix(X[count:count+49],50,num_words)
mydata[[X[count:count+49,]]]
count=count+50
i=i+1
}
while(count<=50){
# author_matrix =matrix(X[count:count+49],50,num_words)
apple = X[count:count+49,]
count=count+50
i=i+1
}
apple <- matrix()
while(count<=50){
# author_matrix =matrix(X[count:count+49],50,num_words)
apple = X[count:count+49,]
count=count+50
i=i+1
}
View(apple)
View(apple)
while(count<=50){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[1,] = X[count:count+49,]
count=count+50
i=i+1
}
w=matrix(0,50,num_words)
while(count<=50){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[1,] = X[count:count+49,]
count=count+50
i=i+1
}
w=matrix(0,50,num_words)
w=matrix(0,1,num_words)
while(count<=50){
# author_matrix =matrix(X[count:count+49],50,num_words)
w[1,] = X[count:count+49,]
count=count+50
i=i+1
}
w=matrix(NA,1,num_words)
View(w)
w=matrix(NA,50,num_words)
View(w)
?matrix
library(tm)
library(foreach)
setwd('/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50')
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
author_dirs = Sys.glob('C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=29)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
labels = append(labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list
file_list
my_corpus
my_corpus$`1`
my_corpus$`1`$content
my_corpus$content
?names
names(my_corpus)
file_list
names(my_corpus) = file_list
file_list[1]
file_list[2]
file_list[2500]
names(my_corpus)
?transpose
names(my_corpus) = t(file_list)
?t
t(file_list)
dim(file_list)
file_list
nrow(file_list)
ncols(file_list)
ncol(file_list)
dim(file_list)
dim(my_corpus)
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
DTM = DocumentTermMatrix(my_corpus)
DTM # some basic summary statistics
class(DTM)  # a special kind of sparse matrix format
inspect(DTM[1:2500,1:50])
X = as.matrix(DTM)
names(my_corpus)
names(my_corpus) = file_list
my_corpus
names(my_corpus)
file_list
AP_train = X[1:45,]
AC_train = X[51:95,]
smooth_count = 1/nrow(X)
w_AP = colSums(AP_train + smooth_count)
w_AP = w_AP/sum(w_AP)
w_AC = colSums(AC_train + smooth_count)
w_AC = w_AC/sum(w_AC)
x_test = X[49,]
head(sort(x_test, decreasing=TRUE), 25)
sum(x_test*log(w_AP))
sum(x_test*log(w_AC))
x_test2 = X[99,]
head(sort(x_test2, decreasing=TRUE), 25)
sum(x_test2*log(w_AP))
sum(x_test2*log(w_AC))
library(randomForest)
install.packages('randomForest')
head(X)
View(X)
transX = T(X)
transX = t(X)
row.names(X) = file_list
View(X)
library(tm)
library(foreach)
library(randomForest)
setwd('/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50')
# Remember to source in the "reader" wrapper function
# it's stored as a Github gist at:
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
## Rolling two directories together into a single corpus
author_dirs_train = Sys.glob('C50train/*')
author_dirs_test = Sys.glob('C50test/*')
file_list = NULL
labels = NULL
for(author in author_dirs_train) {
author_name_train = substring(author, first=29)
files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add_train)
labels = append(labels, rep(author_name_train, length(files_to_add_train)))
}
for(author in author_dirs_test) {
author_name_test = substring(author, first=29)
files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
file_list_test = append(file_list_test, files_to_add_test)
labels = append(labels, rep(author_name_test, length(files_to_add_test)))
}
# Need a more clever regex to get better names here
all_docs_train = lapply(file_list_train, readerPlain)
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))
all_docs_test = lapply(file_list_test, readerPlain)
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))
my_corpus_train = Corpus(VectorSource(all_docs_train))
my_corpus_test = Corpus(VectorSource(all_docs_test))
# Preprocessing
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))
DTM_train = DocumentTermMatrix(my_corpus_train)
DTM_train # some basic summary statistics
DTM_test = DocumentTermMatrix(my_corpus_test)
DTM_test
class(DTM_test)  # a special kind of sparse matrix format
## You can inspect its entries...
inspect(DTM[1:2500,1:50])
DTM = removeSparseTerms(DTM, 0.975)
DTM
# Now a dense matrix
X = as.matrix(DTM_train)
row.names(X) = file_list_train
Y = as.matrix(DTM_test)
row.names(Y) = file_list_test
library(tm)
library(foreach)
library(randomForest)
setwd('/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50')
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
author_dirs_train = Sys.glob('C50train/*')
author_dirs_test = Sys.glob('C50test/*')
file_list = NULL
labels = NULL
for(author in author_dirs_train) {
author_name_train = substring(author, first=29)
files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add_train)
labels = append(labels, rep(author_name_train, length(files_to_add_train)))
}
file_list_test = NULL
labels_test = NULL
file_list_train = NULL
labels_train = NULL
for(author in author_dirs_train) {
author_name_train = substring(author, first=29)
files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add_train)
labels = append(labels, rep(author_name_train, length(files_to_add_train)))
}
for(author in author_dirs_test) {
author_name_test = substring(author, first=29)
files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
file_list_test = append(file_list_test, files_to_add_test)
labels = append(labels, rep(author_name_test, length(files_to_add_test)))
}
all_docs_train = lapply(file_list_train, readerPlain)
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))
all_docs_test = lapply(file_list_test, readerPlain)
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))
my_corpus_train = Corpus(VectorSource(all_docs_train))
my_corpus_test = Corpus(VectorSource(all_docs_test))
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))
DTM_train = DocumentTermMatrix(my_corpus_train)
DTM_train # some basic summary statistics
DTM_test = DocumentTermMatrix(my_corpus_test)
DTM_test
X = as.matrix(DTM_train)
row.names(X) = file_list_train
Y = as.matrix(DTM_test)
row.names(Y) = file_list_test
View(Y)
