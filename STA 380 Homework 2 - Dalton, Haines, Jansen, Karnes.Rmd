---
title: 'STA 380 Homework 2: Dalton, Haines, Jansen, Karnes'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Flights at ABIA
* We know we can ignore the variable 'Year', as all the values for Year will be 2008 
* We want to investigate arrival delays considering delays (ideally we can predict how much later we would arrive if we know how delayed our flight is)
* According to the Federal Aviation Administration a flight is considered delayed if it's departure is 15 minutes later than its scheduled time - this is the standard we will use when analyzing the data

```{r}
abia = read.csv('ABIA.csv')

#Separating the data into delayed and ontime flights
delayed = abia[abia$DepDelay >=15, ]
on.time = abia[abia$DepDelay < 15, ]
```

We want to check a couple aspects of the data now that we have it separated between delayed and ontime: 

* Does the distance being travelled affect the arrival delay?
* How does strongly does departure delay affect arrival delay?
* Which airline has the worst departure delay?


Looking first at distance travelled and its effect on arrival delay:

```{r}
delay.dist = plot(delayed$Distance,delayed$ArrDelay, xlab = 'Distance in Miles', ylab = 'Arrival Delay', main = 'Delayed Flights: Distance vs Arrival Delay')


ontime.dist = plot(on.time$Distance, on.time$ArrDelay, xlab = 'Distance in Miles', ylab = 'Arrival Delay', main = 'Ontime 
Flights: Distance vs Arrival Delay')

```



From these scatter plots, we can see that the range of Arrival Delay for ontime flights has more values below 0 (meaning the flight arrived early) than the delayed flights. However, distance does not seem to affect the arrival delay for both delayed and ontime flights. The scatter plots look almost uniform, meaning that distance is no real indication of an arrival delay. 

We can also see by comparing the two scatterplots that departure delay tends to affect arrival delay as a majority of the flights either arrive ontime or much later. It is much rarer to see a flight arrive early given it has been delayed.

We will calculate the bayesian probability below to put numerical backing to our claims from the graphs. 

```{r}
#Probability a flight arrives early given it has been delayed
#probability early and delayed / probability delayed
early.and.delayed = nrow(delayed[delayed$ArrDelay<0,])/nrow(abia)
prob.delayed = nrow(delayed)/nrow(abia)
early.given.delayed = early.and.delayed/prob.delayed

#Probability a flight arrives early given it is ontime
#probability early and on time / probability ontime
early.and.ontime = nrow(on.time[on.time$ArrDelay<0, ])/nrow(abia)
prob.ontime = nrow(on.time)/nrow(abia)
early.given.ontime = early.and.ontime/prob.ontime

```
* The probability of arriving early given your flight was delayed is: `r early.given.delayed`
* The proability of arriving early given your flight was ontime is: `r early.given.ontime`

This makes logical sense, as it is more difficult to arrive early if you departed late. 


Looking at how departure delay affects arrival delay:

```{r}
#mean arrival delay for delayed flights

arrival.delay = delayed$ArrDelay
#omitting NA values
delay.arrival = na.omit(arrival.delay)
average.arrival.delay = mean(delay.arrival)

#For comparison - what is the mean arrival delay for ontime flights
ontime.arrival = on.time$ArrDelay
#omitting NA values
on.timearrival = na.omit(ontime.arrival)
average.ontime.arrival = mean(on.timearrival)

```
* The average arrival delay in minutes of delayed flights are: `r average.arrival.delay`
* The average arrival delay in minutes of ontime flights are: `r average.ontime.arrival`

So now that we know the average arrival delay of a delayed flight, can we discern the typical arrival delay if we know how delayed a flight is?
```{r}
#Plot of departure delay and arrival delay
the.plot = plot(delayed$DepDelay, delayed$ArrDelay, xlab ='Departure Delay in minutes', ylab = 'Arrival Delay in minutes', main = 'Delayed Flights: Departure Delay vs. Arrival Delay')

```

From the scatter plot above we can see a positive correlation between departure delay and arrival delay. Once again, this makes logical sense as the later you are, the harder it is to arrive on time (or close to on time).


## Author attribution
```{r, echo = FALSE}
options(stringsAsFactors = FALSE)
library(tm)
library(foreach)
library(randomForest)
library(gdata)
library(naivebayes)
library(magrittr)
library(class)
library(caret)
```

```{r}
#Read in data
setwd('/Users/Reeddalton/Documents/GitHub/NBA-Shots-2014-2015/STA-380-Homework-2-Dalton-Haines-Jansen-Karnes/ReutersC50')
```

```{r}
# Create reader function to help with formatting 
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
```

```{r}
## Rolling two directories together into a single corpus
author_dirs_train = Sys.glob('C50train/*')
author_dirs_test = Sys.glob('C50test/*')
anames = list.dirs("C50train", full.names = FALSE)[-1]
file_list_test = NULL
labels_test = NULL
file_list_train = NULL
labels_train = NULL
for(author in author_dirs_train) {
  author_name_train = substring(author, first=10)
  files_to_add_train = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add_train)
  labels = append(labels, rep(author_name_train, length(files_to_add_train)))
}
for(author in author_dirs_test) {
  author_name_test = substring(author, first=10)
  files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add_test)
  labels = append(labels, rep(author_name_test, length(files_to_add_test)))
}
```

```{r}
# Need a more clever regex to get better names here
all_docs_train = lapply(file_list_train, readerPlain) 
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))

all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

my_corpus_train = Corpus(VectorSource(all_docs_train))
my_corpus_test = Corpus(VectorSource(all_docs_test))
```

```{r}
# Preprocessing
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))

# Preprocess test data
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))
```

```{r}
DTM_train = DocumentTermMatrix(my_corpus_train)
DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_train # some basic summary statistics
```

```{r}
## removing sparse terms.

DTM_test = DocumentTermMatrix(my_corpus_test)
DTM_test =removeSparseTerms(DTM_test, 0.95)
```

```{r}
# Now a dense matrix
X = as.matrix(DTM_train)
row.names(X) = file_list_train
X <- X/rowSums(X, na.rm = FALSE)

Y = as.matrix(DTM_test)
row.names(Y) = file_list_test
Y <- Y/rowSums(Y, na.rm = FALSE)

fnames = list.dirs(path = 'C50train/', full.names = FALSE, recursive = TRUE)[-1]

train_mat = t(X)
fnames_rep = rep(anames, each=50)
colnames(train_mat) = fnames_rep
train_df <- as.data.frame(t(train_mat))
train_df$category <- (rownames(train_df))
```

```{r}
#test

test_mat = t(Y)
fnames_rep = rep(anames, each=50)
colnames(test_mat) = fnames_rep
test_df <- as.data.frame(t(test_mat))
test_df$category <- (rownames(test_df))


#test_df <- as.data.frame(test_mat)

model <- naive_bayes(category ~ ., data = train_df, laplace = 1)

preds <- predict(model, newdata = test_df)
conf_mat <- table(preds,train_df$category)
## sum of diagonals vs sum of everything

conf.mat <- confusionMatrix(preds, train_df$category)
conf.mat$overall['Accuracy']
print(conf.mat)



train_filt <- train_df[,colnames(train_df) %in% colnames(test_df)]
author_train <- train_filt$author 
train_filt$author <- NULL

test_filt <- test_df[,colnames(test_df) %in% colnames(train_df)]
actual <- test_filt$author
test_filt$author <- NULL



rffit = randomForest(~.,data= mat, ntree=5,maxnodes=15)
importo = sort(rffit$importance, decreasing = FALSE)

```


## Practice with association rule mining
```{R, echo = FALSE}
library(arules)
library(arulesViz)
```

```{r}
##read in text file (this was in a webpage format)
grocery = readLines("groceries.txt")
```

```{r}
##select lines from file
mypattern = '<td id="([^<]*)" class="blob-code blob-code-inner js-file-line">([^<]*)</td>'
datalines = grep(mypattern,grocery[659:39997],value=TRUE)
```

```{r, echo = FALSE}
##parse through to get desired information
datalines = gsub("<td","",datalines)
datalines = gsub("id=","",datalines)
datalines= gsub("[A-Z0-9]","",datalines)
datalines=gsub("\"\"","",datalines)
datalines = gsub("class=\"","",datalines)
datalines = gsub("blob-code blob-code-inner js-file-line","",datalines)
datalines = gsub("\"","",datalines)
datalines = gsub(">","",datalines)
datalines = gsub("</td","",datalines)
datalines = trimws(datalines, which="left")
length(datalines)

datalines = strsplit(datalines,",")
```


```{r}
###change to transaction format
#' arules transactions
trans = as(datalines, "transactions")

##run apriori and inspect associations
##chose support=.008 to be able to view less frequent items
rules = apriori(trans, 
                      parameter=list(support=.008, confidence=.5, maxlen=6))
inspect(rules)

#We found a threshold of >2.4 for lift allowed us to see the items that increase in likelihood most given their association
#Confidence of >0.58 showed a strong association and gave us the top end of the confidence spectrum
inspect(subset(rules, subset=lift > 2.4 & confidence > 0.58))
```

```{r, echo = FAlSE}
itemFrequencyPlot(trans, topN = 15)
summary(rules)
```
Overall, when observing a wide range of support, lift, and confidence, we found that the rhs was pretty much always whole milk and other vegetables.  This makes sense as these are both items that are kitchen staples and likely purchased on every grocery run.
Whole milk is associated with butter and yogurt, which is to be expected as they are all dairy products.
Lastly, we can see an association in produce where people who purchase root vegetables, citrus fruit, and tropical fruit are 3 times more likely to buy other vegetables. This makes sense as many people who buy these products are interested in health.

```{r}
# Visualize rules
plot(rules)
```

```{r}
# Plot the rules associated by group
plot(rules, method = 'grouped')
```

```{r, echo = FALSE}
# Create subrules
subrules <- head(sort(rules, by = 'lift'), 5)
```

```{r}
# Inspect sample of rules
plot(subrules, method = 'graph')
plot(subrules, method = 'matrix')
plot(sample(rules, 25), method="graph", control=list(layout=igraph::in_circle()))
```